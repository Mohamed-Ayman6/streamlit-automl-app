# -*- coding: utf-8 -*-
"""Prototype

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PRExDk8ES_jRACIplwn7bTfGmOPU8WS6

**ملفات لازم تكون مثبته **
"""



"""**Prototype (Streamlit)**"""

# streamlit automl prototype.py
"""
AutoML + EDA Prototype (Streamlit)
Features:
- Upload file or provide URL (CSV/Excel/JSON/Parquet)
- Fast automatic EDA (head, dtypes, missing, correlation)
- Automatic visualizations for all numeric/categorical features
- Generate full EDA report (HTML via ydata_profiling) and export to PDF (via pdfkit/wkhtmltopdf)
- AutoML run (choice: PyCaret or FLAML) with configurable budget
- Show top models & metrics, allow download of model (.pkl) and generated training code
- Robust validation and simple UX elements (spinners, success/info alerts)
"""

import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import tempfile
import os
import io
import time
import joblib
import textwrap
from pathlib import Path
from typing import Optional, Tuple
# EDA report
from ydata_profiling import ProfileReport
import streamlit.components.v1 as components
# PDF conversion
import pdfkit
# AutoML libs (both supported: PyCaret or FLAML)
# PyCaret is heavier but full-featured; FLAML is lightweight & fast.
try:
    from pycaret.classification import setup as cls_setup, compare_models as cls_compare, pull as cls_pull, save_model as cls_save
    from pycaret.regression import setup as reg_setup, compare_models as reg_compare, pull as reg_pull, save_model as reg_save
    PYCARET_AVAILABLE = True
except Exception:
    PYCARET_AVAILABLE = False

try:
    from flaml import AutoML
    FLAML_AVAILABLE = True
except Exception:
    FLAML_AVAILABLE = False

# Utility functions

def read_any(path_or_buffer: str) -> pd.DataFrame:
    """
    Read CSV/JSON/Excel/Parquet from local path or URL/buffer.
    Returns pandas DataFrame or raises.
    """
    if hasattr(path_or_buffer, "read"):
        try:
            return pd.read_csv(path_or_buffer)
        except Exception:
            path_or_buffer.seek(0)
            try:
                return pd.read_excel(path_or_buffer)
            except Exception as e:
                raise RuntimeError(f"Failed to read uploaded file: {e}")

    src = str(path_or_buffer)
    src_lower = src.lower().strip()
    try:
        if src_lower.startswith(("http://", "https://")):
            # try reading with pandas (csv/json/parquet)
            if src_lower.endswith(".csv") or (".csv" in src_lower and "raw" not in src_lower):
                return pd.read_csv(src)
            if src_lower.endswith(".json"):
                return pd.read_json(src)
            if src_lower.endswith(".parquet"):
                return pd.read_parquet(src)
            if src_lower.endswith((".xls", ".xlsx")):
                return pd.read_excel(src)
            return pd.read_csv(src)
        else:
            p = Path(src)
            if p.exists():
                if p.suffix in [".csv", ".txt"]:
                    return pd.read_csv(src)
                if p.suffix in [".json"]:
                    return pd.read_json(src)
                if p.suffix in [".parquet"]:
                    return pd.read_parquet(src)
                if p.suffix in [".xls", ".xlsx"]:
                    return pd.read_excel(src)
            return pd.read_csv(src)
    except Exception as e:
        raise RuntimeError(f"Failed to read data from {src}: {e}")

def infer_task_type(df: pd.DataFrame, target: Optional[str]) -> str:
    """
    Infer classification or regression based on target column dtype/uniques.
    """
    if not target:
        return "unspecified"
    ser = df[target]
    if pd.api.types.is_numeric_dtype(ser):
        # if few unique values -> classification
        if ser.nunique() <= 20 and ser.nunique() < 0.05 * len(ser):
            return "classification"
        else:
            return "regression"
    else:
        return "classification"

def make_safe_filename(name: str) -> str:
    return "".join(c if c.isalnum() or c in "-_." else "_" for c in name)[:200]

def save_training_snippet(task: str, target_col: str, model_name: str, framework: str="pycaret") -> str:
    """
    Generate a small training script snippet that user can download and customize.
    Returns file path.
    """
    tpl = textwrap.dedent(f"""
    # training_snippet.py
    # Generated snippet: Task={task}, target={target_col}, best_model={model_name}, framework={framework}
    import pandas as pd
    import joblib

    df = pd.read_csv("data.csv")  # replace with your path
    X = df.drop(columns=["{target_col}"])
    y = df["{target_col}"]

    # NOTE: This is a minimal skeleton. For full reproducibility include preprocessing steps.
    """)
    if framework.lower().startswith("pycaret"):
        tpl += textwrap.dedent(f"""
        from pycaret.{ 'classification' if task=='classification' else 'regression'} import setup, create_model, tune_model, finalize_model, save_model

        s = setup(data=df, target="{target_col}", silent=True, session_id=42)
        model = create_model("{model_name}")
        tuned = tune_model(model)
        final = finalize_model(tuned)
        save_model(final, "final_model")
        """)
    elif framework.lower().startswith("flaml"):
        tpl += textwrap.dedent(f"""
        from flaml import AutoML
        automl = AutoML()
        automl.fit(X, y, task='{task}')
        joblib.dump(automl, "flaml_automl.pkl")
        """)
    else:
        tpl += "# Add your training code here\n"

    tmp = Path(tempfile.gettempdir()) / f"training_snippet_{int(time.time())}.py"
    tmp.write_text(tpl, encoding="utf-8")
    return str(tmp)

# Streamlit App

st.set_page_config(page_title="AutoML Platform — Prototype", layout="wide", initial_sidebar_state="expanded")
st.title("🚀 AutoML + EDA Prototype — Streamlit Edition")
st.markdown("**Upload a dataset or paste a URL → Inspect → AutoML → Download artifacts (model, code, report)**")

# Sidebar settings

st.sidebar.header("Settings & Options")
automl_framework = st.sidebar.selectbox("AutoML Framework", options=["pycaret" if PYCARET_AVAILABLE else "pycaret (not installed)",
                                                                     "flaml" if FLAML_AVAILABLE else "flaml (not installed)"])
automl_framework = automl_framework.split()[0].lower()
budget_seconds = st.sidebar.slider("AutoML time budget (seconds)", min_value=10, max_value=600, value=60, step=10)
limit_rows = st.sidebar.number_input("Max rows to load for preview/EDA (0 = all)", value=5000, step=1000)
generate_pdf_opts = st.sidebar.checkbox("Generate PDF from EDA (requires wkhtmltopdf)", value=True)
show_all_visuals = st.sidebar.checkbox("Auto-generate visuals for all columns", value=True)

# Input: upload or URL

st.write("---")
st.header("1) Upload or provide dataset URL")
col1, col2 = st.columns([2, 3])
uploaded_file = col1.file_uploader("Upload file (CSV/Excel/JSON/Parquet)", type=["csv", "xlsx", "xls", "json", "parquet"])
url_input = col2.text_input("Or paste dataset URL (http(s) ... )", value="")

df = None
data_load_error = None

with st.spinner("Reading dataset ..."):
    try:
        if uploaded_file is not None:
            df = read_any(uploaded_file)
        elif url_input.strip():
            df = read_any(url_input.strip())
    except Exception as e:
        data_load_error = str(e)

if data_load_error:
    st.error(f"Failed to load dataset: {data_load_error}")
    st.stop()

if df is None:
    st.info("Upload a file or paste a URL to begin.")
    st.stop()

# Optionally limit rows for preview/EDA (not truncating data used for modeling)

if limit_rows and limit_rows > 0 and len(df) > limit_rows:
    df_preview = df.head(limit_rows).copy()
else:
    df_preview = df.copy()

st.subheader("Dataset Overview")
st.write(f"Rows: **{df.shape[0]}**  —  Columns: **{df.shape[1]}**")
st.dataframe(df_preview.head(10), use_container_width=True)

# Basic info

with st.expander("Column types & missing"):
    info_df = pd.DataFrame({
        "dtypes": df.dtypes.astype(str),
        "n_missing": df.isna().sum(),
        "pct_missing": (df.isna().mean() * 100).round(2),
        "n_unique": df.nunique()
    }).sort_values("pct_missing", ascending=False)
    st.dataframe(info_df, use_container_width=True)

# Quick numeric summary & correlation

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()

col1, col2 = st.columns(2)
with col1:
    st.write("**Numeric summary (head)**")
    if num_cols:
        st.dataframe(df[num_cols].describe().T, use_container_width=True)
    else:
        st.write("No numeric columns found.")

with col2:
    st.write("**Categorical summary (head)**")
    if cat_cols:
        summary_cat = pd.DataFrame({c: df[c].value_counts().head(5) for c in cat_cols}).fillna("")
        st.dataframe(summary_cat, use_container_width=True)
    else:
        st.write("No categorical columns found.")

# Automatic visualizations

st.markdown("---")
st.header("2) Automatic Visualizations (EDA)")

if show_all_visuals:
    # Correlation Heatmap

    if len(num_cols) > 1:
        st.subheader("Correlation Heatmap")
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.heatmap(df[num_cols].corr(), annot=False, cmap="coolwarm", ax=ax)
        st.pyplot(fig, use_container_width=True)

    # Numeric distributions

    if num_cols:
        st.subheader("Numeric distributions")
        for col in num_cols:
            fig = px.histogram(df, x=col, nbins=40, marginal="box", title=f"Distribution: {col}")
            st.plotly_chart(fig, use_container_width=True)

    # Categorical countplots

    if cat_cols:
        st.subheader("Categorical counts (top 20 values)")
        for col in cat_cols:
            top_counts = df[col].value_counts().nlargest(20)
            fig = go.Figure([go.Bar(x=top_counts.index.astype(str), y=top_counts.values)])
            fig.update_layout(title=f"Counts: {col}", xaxis_tickangle=-45, height=300)
            st.plotly_chart(fig, use_container_width=True)
else:
    st.info("Automatic visuals disabled in sidebar.")

# EDA Report generation

st.markdown("---")
st.header("3) Generate full EDA report (HTML + optional PDF)")
if st.button("Generate Full EDA Report"):
    try:
        with st.spinner("Generating profile (this may take a while on large datasets)..."):
            profile = ProfileReport(df_preview, title="Full EDA Report", explorative=True)
            html_path = Path(tempfile.gettempdir()) / f"eda_profile_{int(time.time())}.html"
            profile.to_file(html_path)
        st.success("EDA profile generated.")
        st.subheader("EDA Report (preview)")
        with open(html_path, "r", encoding="utf-8") as f:
            html_text = f.read()
            components.html(html_text, height=800, scrolling=True)
        with open(html_path, "rb") as f:
            st.download_button("⬇️ Download EDA Report (HTML)", f, file_name="eda_report.html", mime="text/html")
        if generate_pdf_opts:
            st.info("Converting HTML → PDF (wkhtmltopdf must be installed on server)...")
            pdf_path = Path(tempfile.gettempdir()) / f"eda_profile_{int(time.time())}.pdf"
            try:
                options = {
                    "quiet": "",
                    "enable-local-file-access": None,
                    "no-outline": None,
                }
                pdfkit.from_file(str(html_path), str(pdf_path), options=options)
                with open(pdf_path, "rb") as f:
                    st.download_button("⬇️ Download EDA Report (PDF)", f, file_name="eda_report.pdf", mime="application/pdf")
                st.success("PDF generated.")
            except Exception as e:
                st.error(f"PDF generation failed: {e}\nMake sure wkhtmltopdf is installed on the host.")
    except Exception as e:
        st.error(f"Failed to generate EDA report: {e}")

st.markdown("---")
st.header("4) Select target & run AutoML")
target_col = st.selectbox("Choose target column (for supervised tasks)", options=[None] + df.columns.tolist(), index=0)

if target_col is None:
    st.warning("No target selected — you can still run unsupervised experiments manually outside this prototype.")
else:
    task = infer_task_type(df, target_col)
    st.info(f"Inferred task: **{task}**")

    st.subheader("AutoML configuration")
    col_a, col_b = st.columns(2)
    with col_a:
        test_size = st.slider("Test size (fraction)", 0.05, 0.5, 0.2, 0.05)
        random_state = st.number_input("Random seed", value=42, step=1)
    with col_b:
        run_baseline = st.checkbox("Run only quick baseline (fast)", value=True)
        show_model_reports = st.checkbox("Show model results table", value=True)

    if st.button("Run AutoML Now"):
        st.info("Preparing data and running AutoML...")
        try:
            from sklearn.model_selection import train_test_split
            df_model = df.copy()
            df_model = df_model[~df_model[target_col].isna()].reset_index(drop=True)
            X = df_model.drop(columns=[target_col])
            y = df_model[target_col]
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
        except Exception as e:
            st.error(f"Failed to prepare data for modeling: {e}")
            st.stop()

        # Run chosen AutoML

        results_df = None
        best_model_obj = None
        model_filename = None
        training_snippet_path = None

        if automl_framework == "pycaret" and PYCARET_AVAILABLE:
            st.write("Running PyCaret AutoML (this may take some time)...")
            try:
                train_df = pd.concat([X_train, y_train], axis=1)
                if task == "classification":
                    s = cls_setup(data=train_df, target=target_col, silent=True, session_id=random_state, verbose=False)
                    best = cls_compare(n_select=5)  # return top 5 models
                    res = cls_pull()
                    results_df = res
                    # best[0] is best model object (if list)
                    if isinstance(best, list):
                        best_top = best[0]
                    else:
                        best_top = best
                    cls_save(best_top, "pycaret_best_model")
                    model_filename = "pycaret_best_model.pkl"
                    joblib.dump(best_top, model_filename)
                    best_model_obj = best_top
                    training_snippet_path = save_training_snippet("classification", target_col, getattr(best_top, "__class__", "model"), framework="pycaret")
                else:
                    s = reg_setup(data=train_df, target=target_col, silent=True, session_id=random_state, verbose=False)
                    best = reg_compare(n_select=5)
                    res = reg_pull()
                    results_df = res
                    if isinstance(best, list):
                        best_top = best[0]
                    else:
                        best_top = best
                    reg_save(best_top, "pycaret_best_model")
                    model_filename = "pycaret_best_model.pkl"
                    joblib.dump(best_top, model_filename)
                    best_model_obj = best_top
                    training_snippet_path = save_training_snippet("regression", target_col, getattr(best_top, "__class__", "model"), framework="pycaret")
            except Exception as e:
                st.error(f"PyCaret AutoML failed: {e}")
        elif automl_framework == "flaml" and FLAML_AVAILABLE:
            st.write("Running FLAML AutoML (fast & lightweight)...")
            try:
                automl = AutoML()
                automl_settings = {
                    "time_budget": budget_seconds,
                    "metric": "accuracy" if task == "classification" else "rmse",
                    "task": "classification" if task == "classification" else "regression",
                    "seed": int(random_state),
                }
                X_train_proc = X_train.copy()
                X_test_proc = X_test.copy()
                for c in X_train_proc.select_dtypes(include=[np.number]).columns:
                    X_train_proc[c] = X_train_proc[c].fillna(X_train_proc[c].median())
                    X_test_proc[c] = X_test_proc[c].fillna(X_train_proc[c].median())
                # encode categorical by factorize (simple)
                for c in X_train_proc.select_dtypes(include=['object', 'category']).columns:
                    X_train_proc[c] = pd.factorize(X_train_proc[c].astype(str))[0]
                    X_test_proc[c] = pd.factorize(X_test_proc[c].astype(str))[0]

                automl.fit(X_train_proc, y_train, **automl_settings)
                best_estimator = automl.model.estimator
                best_model_obj = automl
                model_filename = Path(tempfile.gettempdir()) / f"flaml_automl_{int(time.time())}.pkl"
                joblib.dump(automl, str(model_filename))
                training_snippet_path = save_training_snippet(task, target_col, automl.best_estimator, framework="flaml")
                try:
                    df_res = pd.DataFrame(automl.best_config_history)
                    results_df = df_res
                except Exception:
                    results_df = pd.DataFrame({"best_estimator": [str(automl.model.estimator)]})
            except Exception as e:
                st.error(f"FLAML AutoML failed: {e}")
        else:
            st.error("Selected AutoML framework not available on this environment. Install pycaret or flaml.")
            st.stop()

        if results_df is not None and show_model_reports:
            st.subheader("AutoML Results / Leaderboard")
            try:
                st.dataframe(results_df, use_container_width=True)
            except Exception:
                st.write(results_df)

        # Evaluate and show basic metrics on test set (best effort)
        st.subheader("Evaluation on Hold-out Test Set")
        try:
            if automl_framework == "pycaret" and PYCARET_AVAILABLE and best_model_obj is not None:
                # PyCaret returns wrapped pipeline; use predict
                # ensure X_test columns align (PyCaret pipeline includes preprocessing)
                preds = best_model_obj.predict(X_test)
                # simple metrics
                from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
                if task == "classification":
                    acc = accuracy_score(y_test, preds)
                    f1 = f1_score(y_test, preds, average="macro") if len(np.unique(y_test)) > 1 else None
                    st.write(f"- Accuracy: **{acc:.4f}**")
                    if f1 is not None:
                        st.write(f"- F1 (macro): **{f1:.4f}**")
                else:
                    rmse = mean_squared_error(y_test, preds, squared=False)
                    st.write(f"- RMSE: **{rmse:.4f}**")
            elif automl_framework == "flaml" and FLAML_AVAILABLE and best_model_obj is not None:
                # For FLAML we saved automl object; use predict
                # ensure test processed same way:
                X_test_proc2 = X_test.copy()
                for c in X_test_proc2.select_dtypes(include=[np.number]).columns:
                    X_test_proc2[c] = X_test_proc2[c].fillna(X_train[c].median() if c in X_train else 0)
                for c in X_test_proc2.select_dtypes(include=['object', 'category']).columns:
                    X_test_proc2[c] = pd.factorize(X_test_proc2[c].astype(str))[0]
                preds = best_model_obj.predict(X_test_proc2)
                from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
                if task == "classification":
                    acc = accuracy_score(y_test, preds)
                    f1 = f1_score(y_test, preds, average="macro") if len(np.unique(y_test)) > 1 else None
                    st.write(f"- Accuracy: **{acc:.4f}**")
                    if f1 is not None:
                        st.write(f"- F1 (macro): **{f1:.4f}**")
                else:
                    rmse = mean_squared_error(y_test, preds, squared=False)
                    st.write(f"- RMSE: **{rmse:.4f}**")
            else:
                st.info("Could not evaluate — no model object available or unknown framework.")
        except Exception as e:
            st.error(f"Evaluation failed: {e}")

        st.markdown("---")
        st.header("5) Download artifacts")
        if model_filename and Path(model_filename).exists():
            with open(model_filename, "rb") as f:
                st.download_button("⬇️ Download model (.pkl)", f, file_name=Path(model_filename).name, mime="application/octet-stream")
        elif best_model_obj is not None:
            tmp_model_p = Path(tempfile.gettempdir()) / f"model_{int(time.time())}.pkl"
            try:
                joblib.dump(best_model_obj, str(tmp_model_p))
                with open(tmp_model_p, "rb") as f:
                    st.download_button("⬇️ Download model (.pkl)", f, file_name=tmp_model_p.name, mime="application/octet-stream")
            except Exception as e:
                st.warning(f"Could not serialize model for download: {e}")

        if training_snippet_path and Path(training_snippet_path).exists():
            with open(training_snippet_path, "rb") as f:
                st.download_button("⬇️ Download training snippet (py)", f, file_name=Path(training_snippet_path).name, mime="text/x-python")

        st.success("AutoML run complete. You can re-run with different settings or try another dataset.")

st.write("---")
st.caption("Prototype built by Mohamed gamal othman — Streamlit AutoML prototype. For production: move AutoML to background workers, store artifacts in cloud storage, add authentication and quota controls.")
